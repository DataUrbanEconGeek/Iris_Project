---
title: 'Predicting Iris Species: Clustering'
author: "Andrew Rosa"
date: "5/13/2017"
output: 
        html_document:
                includes:
                        in_header: head.html
                css: style.css
---

### Background

Edgar Anderson was an American botanist. In 1929 he had moved to Britain where he studied and worked at the John Innes Horticultural Institute. It was while studying here that he create a data set containing the sepal lengths, widths, and petal lengths and widths of three different Iris flower species. His coworker and statistician R.A. Fisher then used that data set as an example for statistical methods on classification. This project seeks to demonstrate predicting the classification of an Iris flower's species with the use of clustering algorithms. 

### First

We would normally load in the data, but because this data-set is widely used the CRAN community has already provided the data-set in R's base data-sets package. We can immediately start to use it.

```{r}
iris_df <- iris
str(iris)
```

We can see here that this data-set contains 150 observations. We also see that four of our variables are quantitative and one variable(species) is categorical. Some manipulation will have to be done to the data set before we begin. We're going to remove the species variable for now and save it in a different list for later. 

```{r}
species <- as.list(iris_df$Species)
species <- unlist(species)
iris_df <- iris_df[1:4]
```

### Exploritory Visualizations

We're now ready to look at the data. 

```{r warning=FALSE, echo=FALSE}
library(ggplot2)
library(gridExtra)
library(knitr)
plot_1 <- ggplot(iris_df, aes(x = Petal.Length, y = Petal.Width)) +
                geom_point()
plot_2 <- ggplot(iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +
                geom_point()
plot_3 <- ggplot(iris_df, aes(x = Petal.Length, y = Petal.Width)) +
                stat_density_2d()
plot_4 <- ggplot(iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +
                stat_density_2d()

grid.arrange(plot_1, plot_2, plot_3, plot_4, ncol = 2, nrow = 2)
```

From these plots we can easily see two distinct clusters, but we know from viewing the structure of the data-set above that there are three species of flowers. 

### Hierarchical Clustering

The first type of clustering we'll try out is a Single Hierarchical Clustering. This type of clustering is Agglomerative, which means it takes a bottom up approach. Each observation starts in it's own cluster, pair's are then merged over and over again moving up the cluster. First we'll need to calculate the distance between pairs, this is achieved by using the `dist()` function. We'll use euclidean for the method. This will give use the ordinary straight line distance between two points. After we use the `hclust()` function with the method set to single. Since the function produce an entire tree of every cluster we'll have to cut it to create our 3 clusters to represent the iris species. `cutree()` is used for this.

```{r}
set.seed(3949)
iris_dist <- dist(iris_df, method = "euclidean")
model_1 <- hclust(iris_dist, method = "single")
model_1_cut <- cutree(model_1, 3)
table(species, model_1_cut)
```

```{r echo=FALSE}
plot_5 <- ggplot(iris_df, aes(x = Petal.Length, y = Petal.Width)) +
                geom_point(aes(col = factor(model_1_cut))) +
                theme(legend.position = "none") +
                theme(axis.title.x = element_blank())
plot_6 <- ggplot(iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +
                geom_point(aes(col = factor(model_1_cut))) +
                scale_color_discrete(name = "Clusters     ") +
                theme(axis.title.x = element_blank())
plot_7 <- ggplot(iris_df, aes(x = Petal.Length, y = Petal.Width)) +
                geom_point(aes(col = species)) +
                theme(legend.position = "none")
plot_8 <- ggplot(iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +
                geom_point(aes(col = species))

grid.arrange(plot_5, plot_6, plot_7,  plot_8, ncol = 2, nrow = 2,
             widths = c(2.3, 3.2), heights = c(2.6, 2.8))
```

By interpreting the confusion matrix and visualizations here we see that this clustering method was good at determining one group of flowers, but had a hard time distinguishing between the other two. If we decide to define cluster 1 as setosa, cluster 2 as versicolor, and cluster 3 as virginica, we end up with an accuracy of 68%. Not a very good model at all. We'll now try to use the complete method for hierarchical clustering. This method differs in how it find the shortest distance between to pairs by using the distance of two elements that are farthest away from each other.

```{r}
model_2 <- hclust(iris_dist, method = "complete")
model_2_cut <- cutree(model_2, 3)
table(species, model_2_cut)
```


```{r echo=FALSE}
plot_9 <- ggplot(iris_df, aes(x = Petal.Length, y = Petal.Width)) +
                geom_point(aes(col = factor(model_2_cut))) +
                theme(legend.position = "none") +
                theme(axis.title.x = element_blank()) +
                scale_color_manual(values=c("#F8766D", "#619CFF", "#00BA38"))
plot_10 <- ggplot(iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +
                geom_point(aes(col = factor(model_2_cut))) +
                scale_color_manual(name = "Clusters     ", values=c("#F8766D", "#619CFF", "#00BA38")) +
                theme(axis.title.x = element_blank())
plot_11 <- ggplot(iris_df, aes(x = Petal.Length, y = Petal.Width)) +
                geom_point(aes(col = species)) +
                theme(legend.position = "none")
plot_12 <- ggplot(iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +
                geom_point(aes(col = species))

grid.arrange(plot_9, plot_10, plot_11,  plot_12, ncol = 2, nrow = 2,
             widths = c(2.3, 3.2), heights = c(2.6, 2.8))
```

This model show's a drastic improvement. Defining cluster 1 as setosa, cluster 2 as virginica and cluster 3 as versicolor we have and accuracy of 84%. It may be possible yet though to get better results by using a different clustering algorithm. 

### K-Means Clustering

K-means clustering works by initializing 'k' number of 'means' as random values in a data-set. Clusters are then created by associating every observation to it's nearest mean. A centroid is calculated for each cluster. That centriod becomes the new mean. The process then runs over and over again until convergence has been reached. R's stats package makes this an easy process. We'll simply use the `kmeans()` function, set with 3 centers.

```{r}
set.seed(3949)
model_3 <- kmeans(iris_df, centers = 3)
table(species, model_3$cluster)
```

```{r echo=FALSE}
plot_13 <- ggplot(iris_df, aes(x = Petal.Length, y = Petal.Width)) +
                geom_point(aes(col = factor(model_3$cluster))) +
                theme(legend.position = "none") +
                theme(axis.title.x = element_blank())
plot_14 <- ggplot(iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +
                geom_point(aes(col = factor(model_3$cluster))) +
                scale_color_discrete(name = "Clusters     ") +
                theme(axis.title.x = element_blank())
plot_15 <- ggplot(iris_df, aes(x = Petal.Length, y = Petal.Width)) +
                geom_point(aes(col = species)) +
                theme(legend.position = "none")
plot_16 <- ggplot(iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +
                geom_point(aes(col = species))

grid.arrange(plot_9, plot_10, plot_11,  plot_12, ncol = 2, nrow = 2,
             widths = c(2.3, 3.2), heights = c(2.6, 2.8))
```

It is clear from the visualizations that using the k-means gives us the best result. If we assign setosa to cluster 2, versicolor to cluster 1, and virginica to cluster 3 we get an accuracy of 89.3%. 

### Conclusion

Using the k-means method for clustering works well in separating out the different species from the Iris data-set. From here one could scale the data and then run the k-means method again, but considering that there are not any major outliers in the data-set and the values for the measurements are small, it is doubtful you would make a better model. Instead you could explore further methods of clustering. 

#### Project Source Code

https://github.com/PunkFood-Disme/Iris_Project








